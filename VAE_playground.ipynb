{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7370f90a-85bf-49d9-b6f4-9a41e60211a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/mhuber/Thesis/GitHub/maisi/models/VAE/lima_best.pt\"\n",
    "image_path = \"/home/mhuber/Thesis/data/KermanyV3_resized/test/1/DME-145590-3.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688f31a9-0974-49f0-8050-ac26e2c883ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Checking if files exist...\n",
      "Loading model...\n",
      "Creating model...\n",
      "Loading checkpoint...\n",
      "Available keys in checkpoint: ['epoch', 'autoencoder_state_dict', 'discriminator_state_dict', 'optimizer_g_state_dict', 'optimizer_d_state_dict', 'scheduler_g_state_dict', 'scheduler_d_state_dict', 'best_val_loss', 'config']\n",
      "Found autoencoder_state_dict in checkpoint\n",
      "Model loaded successfully\n",
      "Loading image...\n",
      "Image shape: torch.Size([1, 1, 256, 256]), dtype: torch.float32\n",
      "Processing image with autoencoder...\n",
      "Latent shape: torch.Size([1, 4, 64, 64])\n",
      "Reconstructed shape: torch.Size([1, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2908918/1658160972.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=device.type=='cuda'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results saved to autoencoder_results\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from networks.autoencoderkl_maisi import AutoencoderKlMaisi\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration for the autoencoder from your model\n",
    "config = {\n",
    "    \"spatial_dims\": 2,\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 1,\n",
    "    \"latent_channels\": 4,\n",
    "    \"num_channels\": [64, 128, 256],\n",
    "    \"num_res_blocks\": [2, 2, 2],\n",
    "    \"norm_num_groups\": 32,\n",
    "    \"norm_eps\": 1e-6,\n",
    "    \"attention_levels\": [False, False, False],\n",
    "    \"with_encoder_nonlocal_attn\": False,\n",
    "    \"with_decoder_nonlocal_attn\": False,\n",
    "    \"use_checkpointing\": False,\n",
    "    \"use_convtranspose\": False,\n",
    "    \"norm_float16\": True,\n",
    "    \"num_splits\": 1,\n",
    "    \"dim_split\": 1\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"autoencoder_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and preprocess image\n",
    "def load_image(path, target_size=(256, 256)):\n",
    "    \"\"\"Load and preprocess an image for the autoencoder\"\"\"\n",
    "    img = Image.open(path)\n",
    "    \n",
    "    # Convert to grayscale if input channel is 1\n",
    "    if config[\"in_channels\"] == 1:\n",
    "        img = img.convert(\"L\")\n",
    "    else:\n",
    "        img = img.convert(\"RGB\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * config[\"in_channels\"], [0.5] * config[\"in_channels\"])\n",
    "    ])\n",
    "    \n",
    "    # Create tensor on device\n",
    "    return transform(img).unsqueeze(0).to(device, dtype=torch.float32)\n",
    "\n",
    "# Load model\n",
    "def load_model(path):\n",
    "    \"\"\"Load model from checkpoint, handling nested state dict\"\"\"\n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = AutoencoderKlMaisi(**config).to(device)\n",
    "    \n",
    "    # Load checkpoint with safe loading\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(path, map_location=device, weights_only=True)\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    print(\"Available keys in checkpoint:\", list(checkpoint.keys()))\n",
    "    \n",
    "    if \"autoencoder_state_dict\" in checkpoint:\n",
    "        print(\"Found autoencoder_state_dict in checkpoint\")\n",
    "        model.load_state_dict(checkpoint[\"autoencoder_state_dict\"])\n",
    "    elif \"model\" in checkpoint:\n",
    "        print(\"Found model in checkpoint\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    elif \"state_dict\" in checkpoint:\n",
    "        print(\"Found state_dict in checkpoint\")\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find model weights in checkpoint\")\n",
    "    \n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Basic image processing function that doesn't require complicated SPADE segmentation\n",
    "def process_image():\n",
    "    \"\"\"Process an image with the autoencoder\"\"\"\n",
    "    try:\n",
    "        print(\"Checking if files exist...\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model file not found: {model_path}\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image file not found: {image_path}\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading model...\")\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        print(\"Loading image...\")\n",
    "        img = load_image(image_path)\n",
    "        print(f\"Image shape: {img.shape}, dtype: {img.dtype}\")\n",
    "        \n",
    "        # Get filename without extension for saving results\n",
    "        filename_prefix = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        print(\"Processing image with autoencoder...\")\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=device.type=='cuda'):\n",
    "                # Encode the image\n",
    "                z_mu, z_sigma = model.encode(img)\n",
    "                latent = model.sampling(z_mu, z_sigma)\n",
    "                print(f\"Latent shape: {latent.shape}\")\n",
    "                \n",
    "                # Decode the latent\n",
    "                reconstructed = model.decode(latent)\n",
    "                print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "        \n",
    "        # Visualize and save results\n",
    "        visualize_results(img, latent, reconstructed, filename_prefix)\n",
    "        \n",
    "        print(f\"All results saved to {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def visualize_results(original, latent, reconstructed, filename_prefix):\n",
    "    \"\"\"Visualize and save the results\"\"\"\n",
    "    # 1. Save original and reconstructed images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Original image\n",
    "    orig_img = (original.squeeze().cpu().numpy() + 1) / 2\n",
    "    axes[0].imshow(orig_img, cmap='gray' if config[\"in_channels\"] == 1 else None)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Reconstructed image\n",
    "    recon_img = (reconstructed.squeeze().cpu().numpy() + 1) / 2\n",
    "    axes[1].imshow(recon_img, cmap='gray' if config[\"in_channels\"] == 1 else None)\n",
    "    axes[1].set_title('Reconstructed Image')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{filename_prefix}_reconstruction.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Visualize latent channels\n",
    "    latent_np = latent.squeeze().cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, latent_np.shape[0] + 1, figsize=(20, 4))\n",
    "    \n",
    "    # Plot each channel\n",
    "    for i in range(latent_np.shape[0]):\n",
    "        im = axes[i].imshow(latent_np[i], cmap='viridis')\n",
    "        axes[i].set_title(f'Channel {i+1}')\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    \n",
    "    # Plot the mean of all channels\n",
    "    mean_latent = np.mean(latent_np, axis=0)\n",
    "    im = axes[-1].imshow(mean_latent, cmap='viridis')\n",
    "    axes[-1].set_title('Mean of Channels')\n",
    "    axes[-1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[-1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{filename_prefix}_latent_channels.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Save the latent and mean as numpy arrays\n",
    "    np.save(f\"{output_dir}/{filename_prefix}_latent.npy\", latent_np)\n",
    "    np.save(f\"{output_dir}/{filename_prefix}_mean_latent.npy\", mean_latent)\n",
    "    \n",
    "    # 4. Save individual PNG images of each latent channel\n",
    "    for i in range(latent_np.shape[0]):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(latent_np[i], cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title(f'Latent Channel {i+1}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{filename_prefix}_latent_channel_{i+1}.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Save mean latent as a separate image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(mean_latent, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Mean of Latent Channels')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{filename_prefix}_mean_latent.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    process_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d807ea9-416e-4a4d-a266-4cb8c0424ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
